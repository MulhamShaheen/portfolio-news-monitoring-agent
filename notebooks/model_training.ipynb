{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-02T23:32:30.579903Z",
     "start_time": "2025-07-02T23:32:29.592023Z"
    }
   },
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, \\\n",
    "    DataCollatorForSeq2Seq"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "The [dataset Indian_Financial_News](https://huggingface.co/datasets/kdave/Indian_Financial_News) was chosen for fine-tuning the model. It contains financial news articles in English, which is suitable for the task of summarization. The dataset is already in a structured format with columns for content, summary, and link.\n",
    "\n",
    "As a measure to focus the model on financial news related to stocks, the dataset was filtered to include only articles that contain the word \"stock\" in the content."
   ],
   "id": "d1e708334f54ae4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T23:16:07.877401Z",
     "start_time": "2025-07-02T23:14:57.410002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"hf://datasets/kdave/Indian_Financial_News/training_data_26000.csv\")\n",
    "print(\"Dataset size before filtering:\", len(df))\n",
    "\n",
    "df = df[df[\"Content\"].str.contains(\"stock\", case=False, na=False)]\n",
    "print(\"Dataset size after filtering:\", len(df))"
   ],
   "id": "cf6f1c15b7c7c2a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size before filtering: 26961\n",
      "Dataset size after filtering: 8825\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data parsing from Yahoo Finance\n",
    "\n",
    "For additional data, Yahoo Finance news of top tickers were parsed and cleaned.\n",
    "\n",
    "Using the index S&P 500 Stocks the top tickers were selected for parsing."
   ],
   "id": "97d536d8990cd9cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T23:27:19.778285Z",
     "start_time": "2025-07-02T23:27:19.146697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "tables = pd.read_html(url)\n",
    "\n",
    "sp500_table = tables[0]\n",
    "sp500_tickers = sp500_table[['Symbol', 'Security']].sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print(sp500_tickers.head(10))"
   ],
   "id": "a8ffb477e6562ba1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Symbol              Security\n",
      "0      K             Kellanova\n",
      "1    BRO         Brown & Brown\n",
      "2    LIN             Linde plc\n",
      "3    DTE            DTE Energy\n",
      "4   CINF  Cincinnati Financial\n",
      "5    LHX              L3Harris\n",
      "6    RTX       RTX Corporation\n",
      "7    GLW          Corning Inc.\n",
      "8   BKNG      Booking Holdings\n",
      "9   IDXX    Idexx Laboratories\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After parsing using `services/news_fetcher.py`, the data was cleaned to remove duplicates and non-English characters and saved to csv files in iterative steps (see `notebooks/news_parsing.ipynb`)",
   "id": "fc56c0e212111d1c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T23:31:21.993284Z",
     "start_time": "2025-07-02T23:31:21.448059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "print(\"Parsed news and the Indian Financial News dataset:\", len(df))"
   ],
   "id": "57a556929b3fff3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed news and the Indian Financial News dataset: 10605\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T23:33:56.083370Z",
     "start_time": "2025-07-02T23:33:55.758969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a Dataset object from the DataFrame\n",
    "dataset = Dataset.from_pandas(df)"
   ],
   "id": "9a48c5fe022fb544",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Baseline Evaluation",
   "id": "5dde007a3e2c6503"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As a baseline, the `facebook/bart-large-cnn` model was used, which is a fine-tuned bert-large on CNN Daily Mail dataset for summarization tasks. This model is widely used and serves as a good starting point for comparison with fine-tuned models considering the similarity of financial news to the CNN Daily Mail dataset.",
   "id": "a6f43e6b39dce4e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "baseline_tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "baseline_model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")"
   ],
   "id": "8a462b855ce24f91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess(example):\n",
    "    inputs = baseline_tokenizer(example[\"Content\"], max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "    targets = baseline_tokenizer(example[\"Summary\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)"
   ],
   "id": "bb045630e6add14a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)",
   "id": "aada69416c72b85a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metric = evaluate.load(\"bert_score\", \"en\")\n",
    "\n",
    "def evaluate_baseline_in_batches(dataset, batch_size=16):\n",
    "    baseline_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch = dataset[i:i+batch_size]\n",
    "        input_ids = torch.tensor(batch['input_ids']).to(baseline_model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = baseline_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                max_length=max_target_length,\n",
    "                num_beams=4\n",
    "            )\n",
    "        preds = baseline_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        labels = baseline_tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)\n",
    "    return all_preds, all_labels\n",
    "\n",
    "eval_dataset = tokenized_dataset['test']\n",
    "preds, labels = evaluate_baseline_in_batches(eval_dataset)\n",
    "\n",
    "decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in preds]\n",
    "decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in labels]\n",
    "result = metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "aggregated = {\n",
    "    \"precision\": float(np.mean(result[\"precision\"])),\n",
    "    \"recall\":    float(np.mean(result[\"recall\"])),\n",
    "    \"f1\":        float(np.mean(result[\"f1\"])),\n",
    "}\n",
    "print(aggregated)"
   ],
   "id": "2213594956c73c61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The evaluation results for the baseline model were calculated using Google Colab's T4 GPU\n",
    "\n",
    "```\n",
    "{\n",
    " 'eval_precision': 0.8868377528808735,\n",
    " 'eval_recall': 0.8776564202926777,\n",
    " 'eval_f1': 0.8821186708079444,\n",
    "}\n",
    "```"
   ],
   "id": "c1916c08e5072eb1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Fine-Tuning\n",
    "\n",
    "For open lightweight domain-focused models for summarization tasks the following models were selected:\n",
    "- [bert-small-finetuned-cnn](https://huggingface.co/mrm8488/bert-small2bert-small-finetuned-cnn_daily_mail-summarization) 29 M params This model is a warm-started BERT2BERT model fine-tuned on the CNN/Dailymail_summarization dataset.\n",
    "- [Falconsai/text_summarization](https://huggingface.co/Falconsai/text_summarization) 60,5 лю params Fine-Tuned T5 Small designed for the task of text summarization."
   ],
   "id": "3d70b68dcf017f5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Function to use for evaluation in the Trainer\n",
    "def get_metic_func(tokenizer):\n",
    "    def compute_metrics(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "\n",
    "        # decode preds and labels\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # bertscore expects newline after each sentence\n",
    "        decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "        decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "        result = metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "\n",
    "        aggregated = {\n",
    "            \"precision\": float(np.mean(result[\"precision\"])),\n",
    "            \"recall\":    float(np.mean(result[\"recall\"])),\n",
    "            \"f1\":        float(np.mean(result[\"f1\"])),\n",
    "        }\n",
    "\n",
    "        return aggregated\n",
    "    return compute_metrics"
   ],
   "id": "e74f2c555a6bda37"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### bert-small-finetuned-cnn",
   "id": "a90e74201df2592e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T23:50:46.835212Z",
     "start_time": "2025-07-02T23:50:46.831983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/bert-small2bert-small-finetuned-cnn_daily_mail-summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/bert-small2bert-small-finetuned-cnn_daily_mail-summarization\")"
   ],
   "id": "b0dc28cf731ff880",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess(example):\n",
    "    inputs = tokenizer(example[\"Content\"], max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(example[\"Summary\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)"
   ],
   "id": "fbadbb813ea99172"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)",
   "id": "52455490546011e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    logging_dir='./logs',\n",
    ")"
   ],
   "id": "2046c716347f8e66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=get_metic_func(tokenizer)\n",
    ")"
   ],
   "id": "cef1a4317e554f2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.train()",
   "id": "1594d20c1b8614fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.evaluate()",
   "id": "41bb8bc4533cd536"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The training and evaluation results for the model `bert-small-finetuned-cnn` were calculated using Google Colab's T4 GPU\n",
    "\n",
    "```\n",
    "{\n",
    "     'eval_loss': 0.1591629683971405,\n",
    "     'eval_precision': 0.9290257127231079,\n",
    "     'eval_recall': 0.9352621863981861,\n",
    "     'eval_f1': 0.9320505570384101,\n",
    "     'eval_runtime': 545.7026,\n",
    "     'epoch': 5.0\n",
    " }\n",
    "```"
   ],
   "id": "a5159aa6b3ba5ac5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Falconsai/text_summarization",
   "id": "779598a81929ba3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Falconsai/text_summarization\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Falconsai/text_summarization\")"
   ],
   "id": "7fc7d012c211b844",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess(example):\n",
    "    inputs = tokenizer(example[\"Content\"], max_length=max_input_length, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(example[\"Summary\"], max_length=max_target_length, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)"
   ],
   "id": "7e72122b9daf8df5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tokenized_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)",
   "id": "d473683258b8cc3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"results\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=5,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    logging_dir='./logs',\n",
    ")"
   ],
   "id": "2a5cfa9defaca0a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=get_metic_func(tokenizer)\n",
    ")\n"
   ],
   "id": "e77a84eb51bfd126"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.train()",
   "id": "1c9f259b1835de9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.evaluate()",
   "id": "d2ff1aefb8946efd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The training and evaluation results for the model `falconsai/text_summarization` were calculated using Google Colab's T4 GPU\n",
    "\n",
    "```\n",
    "{\n",
    " 'eval_loss': 0.22356651723384857,\n",
    " 'eval_precision': 0.9129834810936942,\n",
    " 'eval_recall': 0.839822987892206,\n",
    " 'eval_f1': 0.8745714848599347,\n",
    " 'eval_runtime': 137.4924,\n",
    " 'epoch': 5.0\n",
    "}\n",
    "```"
   ],
   "id": "cddbedb931960557"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "The fine-tuning of the summarization models on our combined dataset has shown promising results. The `bert-small-finetuned-cnn` model achieved a high F1 score of approximately 0.932, while the `falconsai/text_summarization` model achieved a slightly lower F1 score of approximately 0.875.\n",
    "\n",
    "Both fine-tuned models outperformed the baseline model, indicating that domain-specific fine-tuning can significantly enhance summarization performance in the financial news domain.\n",
    "\n",
    "\n",
    "| Model                                   | Precision | Recall   | F1      | Loss      | Epochs | Runtime (s) |\n",
    "|------------------------------------------|-----------|----------|---------|-----------|--------|-------------|\n",
    "| facebook/bart-large-cnn (Baseline)      | 0.8868    | 0.8777   | 0.8821  | -         | -      | -           |\n",
    "| bert-small-finetuned-cnn                 | 0.9290    | 0.9353   | 0.9321  | 0.1592    | 5      | 545.70      |\n",
    "| falconsai/text_summarization             | 0.9130    | 0.8398   | 0.8746  | 0.2236    | 5      | 137.49      |\n"
   ],
   "id": "e4585b0ad8bab643"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
